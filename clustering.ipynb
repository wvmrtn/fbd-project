{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import regex as re\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "# third-party libraries\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.multiprocessing\n",
    "from dask.diagnostics import ProgressBar\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, MeanShift, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# local libraries\n",
    "from fbd import read_parquet, plot_dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose cluster method\n",
    "cluster_method = ['kmeans', 'agglomerative_ward', \n",
    "                  'agglomerative_average', 'agglomerative_complete',\n",
    "                  'meanshift', 'spectral'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose factor model\n",
    "num_feat = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob(f'data/loadings/{num_feat}_factor/*.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [re.findall(r'\\d{4}-\\d{2}-\\d{2}', f)[0] for f in filenames]\n",
    "dates_dt = pd.to_datetime(dates).sort_values()\n",
    "dates_str = dates_dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filenames again in correct order\n",
    "filenames = [glob(f'data/loadings/{num_feat}_factor/{d}.parquet')[0] for d in dates_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_mat = pd.read_csv('data/russell3000.csv.gz', compression='gzip',\n",
    "                        index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "permnos = const_mat.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean company information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv('data/permno_info.csv.gz', compression='gzip',\n",
    "                   index_col=0)\n",
    "\n",
    "info['permno'] = info['permno'].astype(int).astype(str)\n",
    "# get rid of nan in naics\n",
    "info.dropna(how='any', subset=['naics'], inplace=True)\n",
    "# remove duplicate rows from info\n",
    "info.drop_duplicates(subset=['permno'], inplace=True, keep='last', ignore_index=True)\n",
    "\n",
    "# get sector from naics\n",
    "info['naics'] = info['naics'].astype(int).astype(str)\n",
    "info['sector'] = info['naics'].str[:2]\n",
    "info[['naics', 'sector']] = info[['naics', 'sector']].astype(int)\n",
    "\n",
    "# we won't be needing siccd actually\n",
    "info.drop(columns=['siccd'], inplace=True)\n",
    "\n",
    "# set permno to index\n",
    "info.set_index('permno', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comnam     0\n",
       "naics      0\n",
       "ticker    12\n",
       "sector     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14643',\n",
       " '14271',\n",
       " '19084',\n",
       " '13250',\n",
       " '14247',\n",
       " '18807',\n",
       " '91638',\n",
       " '14669',\n",
       " '19148',\n",
       " '18363',\n",
       " '15120',\n",
       " '17782',\n",
       " '15829',\n",
       " '92615',\n",
       " '14065',\n",
       " '18579',\n",
       " '16877',\n",
       " '92494',\n",
       " '14712',\n",
       " '14789',\n",
       " '18151',\n",
       " '15044']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing info securities\n",
    "missing_permnos = list(set(permnos) - set(info.index.values))\n",
    "missing_permnos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sectors = len(info['sector'].unique())\n",
    "num_sectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clusters(i):\n",
    "    f = filenames[i]\n",
    "    t = dates_str[i]\n",
    "    loadings = read_parquet(f)\n",
    "    loadings.dropna(how='all', axis=1, inplace=True)\n",
    "    # get permnos as observations to cluster\n",
    "    loadings_t = loadings.T\n",
    "    # perform agglomerative clustering using ward linkage\n",
    "    if cluster_method == 'kmeans':\n",
    "        ac = KMeans(n_clusters=num_sectors, random_state=0)\n",
    "    elif cluster_method == 'agglomerative_ward':\n",
    "        ac = AgglomerativeClustering(n_clusters=num_sectors, linkage='ward')\n",
    "    elif cluster_method == 'agglomerative_average':\n",
    "        ac = AgglomerativeClustering(n_clusters=num_sectors, linkage='average')\n",
    "    elif cluster_method == 'agglomerative_complete':\n",
    "        ac = AgglomerativeClustering(n_clusters=num_sectors, linkage='average')\n",
    "    elif cluster_method == 'meanshift':\n",
    "        ac = MeanShift()\n",
    "    elif cluster_method == 'spectral':\n",
    "        ac = SpectralClustering(n_clusters=num_sectors, random_state=0)\n",
    "    #loadings_t_trans = StandardScaler().fit_transform(loadings_t)\n",
    "    ac = ac.fit(loadings_t)\n",
    "    #ac = ac.fit(loadings_t_trans)\n",
    "    labels = pd.DataFrame(columns=[t], index=loadings_t.index, data=ac.labels_)\n",
    "    # filter loadings according to index in info (get rid of permnos that are in loadings and not in info)\n",
    "    labels = labels[labels.index.isin(info.index)]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f146f7bdee0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# default setting for Dask\n",
    "dask.config.set(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#                                       ] | 4% Completed |  1min 42.5s"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with ProgressBar():\n",
    "    promises = [dask.delayed(compute_clusters)(i) for i, _ in enumerate(filenames)]\n",
    "    all_labels = dask.compute(promises)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_labels = pd.concat([info] + all_labels, axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to parquet\n",
    "info_labels.columns = info_labels.columns.astype(str)\n",
    "info_labels.index = info_labels.index.astype(str)\n",
    "info_labels.to_parquet(f'data/clustering/{num_feat}_factor/{cluster_method}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some loadings\n",
    "f = filenames[0]\n",
    "t = dates_str[0]\n",
    "loadings = read_parquet(f)\n",
    "loadings.dropna(how='all', axis=1, inplace=True)\n",
    "loadings_t = loadings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(n_clusters=None, distance_threshold=0)\n",
    "ac = ac.fit(loadings_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "plot_dendrogram(ac, truncate_mode='level', p=4, ax=ax, color_threshold=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
